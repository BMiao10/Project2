{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import random\n",
    "# from clusters import algs\n",
    "# TODO: add to requirements\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"talk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    \"\"\"\n",
    "    Parent class for hierarchical and partitioning clustering\n",
    "    \n",
    "    Attributes:\n",
    "        clusters (list) : initialize an empty vector to contain cluster labels for clustering\n",
    "        \n",
    "    Params:\n",
    "        k () : number of clusters to initialize\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Constructor class for clustering\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.clusters = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_dist(values1, values2, how=\"bit\"):\n",
    "        \"\"\"\n",
    "        Calculates the distance between two arrays\n",
    "        \n",
    "        Params:\n",
    "            values1 (np.array): First array of values to calculate distance between\n",
    "            values2 (np.array): Second array of values to calculate distance between\n",
    "            how (str): Specifies what metric to return (\"ham\"ming, \"euc\"lidean, jac\"card {for sparse data}, or \"bit\"-wise jaccard)\n",
    "        \n",
    "        Returns:\n",
    "            Distance (euclidian, manhattan, or jaccard) between the 2 arrays\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: Sample value arrays do not have the same length\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate jaccard distance using array values with *sparse* representations.\n",
    "        # The value arrays do not have to be the same length\n",
    "        if how==\"jac\" : \n",
    "            return 1 - (len(np.intersect1d(values1, values2)) / len(np.union1d(values1, values2)))\n",
    "            \n",
    "        # For all other types of clustering, check that value arrays are the same length\n",
    "        if len(values1) != len(values2):\n",
    "            raise ValueError(\"Sample values must have same length to calculate distance by %s\"%how)\n",
    "\n",
    "        # Calculations for other distance metrics - must have dense representation\n",
    "        if how==\"ham\" : \n",
    "            return np.bitwise_xor(values1, values2).sum()\n",
    "            \n",
    "        elif how==\"bit\" : \n",
    "            \n",
    "            if np.bitwise_or(values1, values2).sum() == 0: \n",
    "                return 0\n",
    "            \n",
    "            return 1 - ((np.bitwise_and(values1, values2).sum() / np.bitwise_or(values1, values2).sum()))\n",
    "        \n",
    "        elif how==\"euc\":\n",
    "            return math.sqrt(np.square(values1 - values2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ligand():\n",
    "    \"\"\"\n",
    "    Object containing representation and description of molecule\n",
    "    \n",
    "    Attributes:\n",
    "        ligand_id (int): a unique ID to track the ligand\n",
    "        score (list): Vina AutoDock score\n",
    "        smile (str): SMILE-formatted string representation\n",
    "        onbits (list, set, other iterable): sparse representation of ligand fingerprint\n",
    "        \n",
    "    Params:\n",
    "        ligand_id (int) : a unique ID to track the ligand\n",
    "        score (list) : Vina AutoDock score\n",
    "        smile (str) : SMILE-formatted string representation\n",
    "        onbits (list, set, other iterable) : sparse representation of ligand fingerprint\n",
    "        bit_length (int, default=1024) : the length of the bit vector for densification\n",
    "        densify (bool, default=True) : whether to densify the onbits upon initialization\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ligand_id, score, smile, onbits, bit_length=1024, densify=True):\n",
    "        \"\"\"\n",
    "        Constructor method for Ligand class\n",
    "        \"\"\"\n",
    "        self.ligand_id = ligand_id\n",
    "        self.score = score\n",
    "        self.smile = smile\n",
    "        self.onbits = onbits\n",
    "        self.dense = False\n",
    "        \n",
    "        if densify:\n",
    "            self.densify(bit_length)\n",
    "            self.dense = True\n",
    "\n",
    "    def densify(self, length):\n",
    "        \"\"\"\n",
    "        Converts sparse representation of ligand onbits to dense binary array\n",
    "        \n",
    "        Params:\n",
    "            length (int) : the length of the dense representation\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.dense: \n",
    "            print(\"onbits already dense\")\n",
    "        else:\n",
    "            all_bits = self.onbits\n",
    "            self.onbits = np.zeros(length, dtype=int)\n",
    "            for bit in all_bits: \n",
    "                self.onbits[bit] = 1\n",
    "            self.dense = True\n",
    "        \n",
    "    def sparsify(self):\n",
    "        \"\"\"\n",
    "        Converts dense representation of ligand onbits to sparse array of indices where onbits occur\n",
    "        \"\"\"\n",
    "        if not self.dense: \n",
    "            print(\"onbits already sparse\")\n",
    "        else: \n",
    "            self.onbits = np.where(self.onbits == 1)\n",
    "            \n",
    "def readLigands(ligand_file, n=None):\n",
    "    \"\"\"\n",
    "    Reads in csv file containing ligand information and converts data to list of Ligand objects\n",
    "    \n",
    "    Params:\n",
    "       ligand_file (str, pathlike) : csv file containing ligand information\n",
    "       n (int, default=None) : number of ligands to read in, default reads in all ligands in file\n",
    "       \n",
    "    Returns:\n",
    "        list of Ligand objects\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ligands = pd.read_csv(ligand_file)\n",
    "    \n",
    "    if n is not None: \n",
    "        ligands = ligands.loc[:(n-1)]\n",
    "\n",
    "    ligands_list = []\n",
    "    for ind in list(ligands.index):\n",
    "\n",
    "        curr = ligands.loc[ind]\n",
    "        onbits = [int(obj) for obj in curr[\"OnBits\"].split(\",\")]\n",
    "\n",
    "        object_ligands = Ligand(curr[\"LigandID\"], curr[\"Score\"], curr[\"SMILES\"], onbits)\n",
    "        ligands_list.append(object_ligands)\n",
    "        \n",
    "    return ligands_list\n",
    "\n",
    "def preprocessLigands(samples):\n",
    "    \"\"\"\n",
    "    Extracts onbits information from ligands as a preprocessing step to clustering or calculating other scores\n",
    "    \n",
    "    Params:\n",
    "       samples (list(Ligands)) : list of Ligand objects with dense onbit representations\n",
    "       \n",
    "    Returns:\n",
    "        np.ndarray containing samples x onbits\n",
    "        \n",
    "    \"\"\"\n",
    "    onbits_list = []\n",
    "        \n",
    "    for curr_sample in samples:\n",
    "        onbits_list.append(curr_sample.onbits)\n",
    "        \n",
    "    return np.stack(onbits_list, axis=0)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitionClustering(Clustering):\n",
    "    \"\"\"\n",
    "    Perform partition clustering for a set of sample values\n",
    "    \n",
    "    Attributes:\n",
    "        Clustering().__init__ (inherits) : attributes from Clustering\n",
    "        centroids (dict) : initialize empty dictionary to contain centroids\n",
    "        \n",
    "    Params:\n",
    "        k () : number of clusters to initialize\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Constructor class for partition clustering\n",
    "        Inherits from Clustering class\n",
    "        \"\"\"\n",
    "        super(PartitionClustering, self).__init__(k) \n",
    "        self.centroids = {}\n",
    "   \n",
    "    def cluster(self, samples, max_iter=10, dist_met=\"euc\", preprocess=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform partitioning (kmeans++ & kmeans) clustering given a set of samples such as ligands\n",
    "\n",
    "        Params:\n",
    "            samples (list, np.array, set, or other iterable object) : Set of ligands to cluster\n",
    "            max_iter (int, default=10) : maximum number of iterations for clustering if the algorithm does not converge\n",
    "            dist_met (str, default=\"euc\") : distance metric to use for calculating distance between points and centroids\n",
    "            preprocess (method, default=None) :  function that formats sample values to array / array-like object for clustering\n",
    "        \n",
    "        Returns:\n",
    "            list of cluster labels corresponding to orginal sample list\n",
    "            \n",
    "        Raises:\n",
    "            ValueError : If number of clusters greater than number of samples \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check that the number of clusters does not exceed number of samples\n",
    "        if self.k > len(samples):\n",
    "            raise ValueError(\"Cannot have more clusters than samples!\")\n",
    "        \n",
    "        # extract onbits from Ligand objects and put them into an ndarray\n",
    "        if preprocess is not None: \n",
    "            onbits = preprocess(samples)\n",
    "        \n",
    "        ### Use k-means++ to initialize self.k random centroids, with inspiration from\n",
    "        ### https://www.real-statistics.com/multivariate-statistics/cluster-analysis/initializing-clusters-k-means/\n",
    "        \n",
    "        # Store a copy of the sample value data for k-means++\n",
    "        # because values will be deleted from array as centroids are selected\n",
    "        cent_samples = onbits.copy()\n",
    "        \n",
    "        # 1. Choose a random data point as first centroid c0 (and remove from further centroid selection)\n",
    "        # Centroids are stored as index:sample for easy access\n",
    "        rand = random.randint(0, len(cent_samples)-1)\n",
    "        self.centroids[0] = cent_samples[rand]\n",
    "        cent_samples = np.delete(cent_samples, rand, 0)\n",
    "        \n",
    "        # 2. For each data points that is not a centroid, find min distance to all chosen centroids\n",
    "        # Each iteration, choose new centroid with random probability based on distance\n",
    "        # Repeat until k centroids are selected \n",
    "        while len(self.centroids.keys()) < self.k:\n",
    "            \n",
    "            # get minimum distance from each sample to the centroids\n",
    "            distances = []\n",
    "            for curr_sample in cent_samples: \n",
    "                \n",
    "                min_dist = np.inf\n",
    "                for curr_cent in self.centroids.keys():\n",
    "                    curr_dist = super().calc_dist(curr_sample, self.centroids[curr_cent], how=dist_met)\n",
    "                    \n",
    "                    if curr_dist < min_dist:\n",
    "                        min_dist = curr_dist\n",
    "                        \n",
    "                distances.append(min_dist)\n",
    "            \n",
    "            # Normalize array of minimum distance values to 1 to create \"probablilities\"\n",
    "            # The lower the distance (ie. the closer it is to a current centroid), the lower the probability\n",
    "            # Values with a distance of 0 (identical to current centroid) will not be selected\n",
    "            prob = np.array(distances)\n",
    "            prob = prob / prob.sum()\n",
    "            \n",
    "            # Draw a random choice based on the probabilities\n",
    "            choice = np.random.choice(range(len(cent_samples)), 1, p=prob)\n",
    "            \n",
    "            # Add the random choice of sample to centroids and remove it from further consideration \n",
    "            # I realized after writing this that it's not neccessary to delete the sample\n",
    "            # since it will have 0 chance of being selected again, but it would decrease runtime \n",
    "            # (a tiny bit) during min distance calculations\n",
    "            self.centroids[len(self.centroids.keys())] = cent_samples[choice[0]]\n",
    "            cent_samples = np.delete(cent_samples, choice[0], 0)\n",
    "\n",
    "        ### Use chosen centroids to perform k-means clustering\n",
    "        \n",
    "        # Keep track of ind for the number of iterations \n",
    "        # and not_converged to see whether algo has converged\n",
    "        ind = 0\n",
    "        not_converged = True\n",
    "        \n",
    "        # assign each sample to a cluster\n",
    "        self.clusters = [0] * len(onbits)\n",
    "        \n",
    "        # clustering - adapted from https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "        # https://medium.com/@rishit.dagli/build-k-means-from-scratch-in-python-e46bf68aa875\n",
    "        while ind < max_iter and not_converged:\n",
    "            \n",
    "            # calculate the distance from each sample to the centroid\n",
    "            for sample_ind in range(len(onbits)):\n",
    "                \n",
    "                min_dist = np.inf\n",
    "                for cent_ind in self.centroids.keys():\n",
    "                    curr_dist = super().calc_dist(onbits[sample_ind], self.centroids[cent_ind], how=dist_met)\n",
    "                    \n",
    "                    # assign to lowest centroid\n",
    "                    if curr_dist < min_dist:\n",
    "                        self.clusters[sample_ind] = cent_ind\n",
    "                        min_dist = curr_dist\n",
    "            \n",
    "            # if any of the centroids are empty, fix by assigning a random point to that centroid\n",
    "            cent_keys, counts = np.unique(self.clusters, return_counts=True)\n",
    "            cent_dict = dict(zip(cent_keys, counts))\n",
    "            \n",
    "            for centroid in self.centroids.keys():\n",
    "                \n",
    "                # search for centroids that are empty\n",
    "                if centroid not in self.clusters: \n",
    "                    # put the first point from another cluster (that has \n",
    "                    # more than one point) into this current cluster\n",
    "                    for index in range(len(self.clusters)):\n",
    "                        if cent_dict[self.clusters[index]] > 1:\n",
    "                            self.clusters[index] = centroid\n",
    "                            break\n",
    "                \n",
    "            # store the previous clusters to check for convergence after updating\n",
    "            prev_centroids = self.centroids\n",
    "            \n",
    "            # calculate mean of new clusters and update centroids\n",
    "            for curr_cent in self.centroids.keys():\n",
    "                \n",
    "                # get the onbits for the centroids \n",
    "                curr_indices = list(np.where(np.array(self.clusters) == curr_cent)[0])\n",
    "                curr_onbits = np.take(onbits, curr_indices, axis=0)\n",
    "                \n",
    "                # take the mean of the selected onbits and set that as the new centroid\n",
    "                new_centroid = np.zeros(curr_onbits.shape[1])\n",
    "                \n",
    "                for curr_onbit in curr_onbits:\n",
    "                    new_centroid += curr_onbit\n",
    "                    \n",
    "                self.centroids[centroid] = new_centroid / len(curr_onbits)\n",
    "            \n",
    "            # Check for convergence by comparing the old to new centroids\n",
    "            # If old centroids are the same, then the algorithm is stopped\n",
    "            # Adapted from https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "            all_equal = 0\n",
    "            \n",
    "            for key in self.centroids.keys():\n",
    "                if (prev_centroids[key] == self.centroids[key]).all():\n",
    "                    all_equal += 1\n",
    "                    \n",
    "            if all_equal == len(self.centroids.keys()):\n",
    "                not_converged = False\n",
    "        \n",
    "            # increment index tracking the number of iterations\n",
    "            ind += 1\n",
    "        \n",
    "        return self.clusters\n",
    "    \n",
    "class HierarchicalClustering(Clustering):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering for a set of sample values  \n",
    "    \n",
    "    Attributes:\n",
    "        Clustering().__init__ (inherits) : attributes from Clustering\n",
    "        \n",
    "    Params:\n",
    "        k () : number of clusters to initialize\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Constructor class for hierarchical clustering\n",
    "        \"\"\"\n",
    "        super(HierarchicalClustering, self).__init__(k)\n",
    "    \n",
    "    def cluster(self, samples, linkage = \"max\", dist_met=\"bit\", preprocess=None): \n",
    "        \n",
    "        \"\"\"\n",
    "        Perform hierarchical clustering given a set of samples such as ligands\n",
    "\n",
    "        Params:\n",
    "            samples (list, np.array, set, or other iterable object) : Set of ligands to cluster\n",
    "            linkage (str, default=\"max\") : type of linkage to use when updating the distance matrix\n",
    "            dist_met (str, default=\"bit\") : distance metric to use for calculating distance between points and centroids\n",
    "            preprocess (method, default=None) :  function that formats sample values to array / array-like object for clustering\n",
    "        \n",
    "        Returns:\n",
    "            list of cluster labels corresponding to orginal sample list\n",
    "            \n",
    "        Raises:\n",
    "            ValueError : If number of clusters greater than number of samples \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Check that the number of clusters does not exceed number of samples\n",
    "        if self.k > len(samples):\n",
    "            raise ValueError(\"Cannot have more clusters than samples!\")\n",
    "            \n",
    "        # extract onbits from Ligand objects and put them into an ndarray\n",
    "        if preprocess is not None: \n",
    "            samples = preprocess(samples)\n",
    "\n",
    "        # Create matrix and labels list.\n",
    "        # Initialize to infinity so all minimum distances \n",
    "        # are less than the default \n",
    "        dist_mat = np.ones([len(samples), len(samples)]) * np.inf\n",
    "        \n",
    "        # keep track of which matrix index values maps to which sample\n",
    "        samples_dict = dict(zip(range(len(samples)), samples))\n",
    "        \n",
    "        # create distance matrix\n",
    "        for ind1 in samples_dict.keys():\n",
    "            for ind2 in samples_dict.keys():\n",
    "                if ind1 != ind2:\n",
    "                    \n",
    "                    dist = super().calc_dist(samples_dict[ind1], samples_dict[ind2], how=dist_met)\n",
    "                        \n",
    "                    dist_mat[ind1][ind2] = np.inf\n",
    "                    dist_mat[ind2][ind1] = dist\n",
    "        \n",
    "        # assign each sample to its own cluster first\n",
    "        self.clusters = [[ind] for ind in samples_dict.keys()]\n",
    "        \n",
    "        # iterate until the number of clusters is reached\n",
    "        while len(self.clusters) > self.k:\n",
    "\n",
    "            # get the index of the minimum value \n",
    "            min_x = np.where(dist_mat == np.min(dist_mat))[0][0]\n",
    "            min_y = np.where(dist_mat == np.min(dist_mat))[1][0]\n",
    "\n",
    "            # Update the linkage for the two clusters\n",
    "            # For single linkage, -1 is used for values in the bottom triangle instead\n",
    "            # of np.inf, so that these values will be ignored\n",
    "            for i in range(len(dist_mat)):\n",
    "                if min_x != i & min_y !=i:\n",
    "                    if linkage == 'average': # average linkage\n",
    "                        dist_mat[i][min_x] = (dist_mat[i][min_x] + dist_mat[i][min_y]) / 2.0\n",
    "                    elif linkage == 'min': # single linkage\n",
    "                        dist_mat = np.where(dist_mat == np.inf, -1, dist_mat)\n",
    "                        dist_mat[i][min_x] = min(dist_mat[i][min_x], dist_mat[i][min_y])\n",
    "                        dist_mat = np.where(dist_mat == -1, np.inf, dist_mat)\n",
    "                    elif linkage == 'max': # complete linkage\n",
    "                        dist_mat[i][min_x] = max(dist_mat[i][min_x], dist_mat[i][min_y])\n",
    "                    \n",
    "            # get the clusters with minimum distance\n",
    "            cluster_x = self.clusters[min_x]\n",
    "            cluster_y = self.clusters[min_y]\n",
    "            \n",
    "            # combine the clusters and store the new cluster at index min_x (replacing it)\n",
    "            self.clusters[min_x] = cluster_x + cluster_y\n",
    "            \n",
    "            # remove the old cluster that updated\n",
    "            self.clusters.pop(min_y)\n",
    "            dist_mat = np.delete(dist_mat, min_y, axis=0)\n",
    "            dist_mat = np.delete(dist_mat, min_y, axis=1)\n",
    "            \n",
    "        # create dictionaries to map cluster labels to samples\n",
    "        #sample_to_cluster = {}\n",
    "        ind_to_clust = {}\n",
    "\n",
    "        # assign cluster labels to samples\n",
    "        for clust in range(len(self.clusters)):\n",
    "            for ind in self.clusters[clust]:\n",
    "                #sample_to_cluster[samples_dict[ind]] = clust\n",
    "                ind_to_clust[ind] = clust\n",
    "        \n",
    "        # return vector of cluster labels for each sample\n",
    "        self.clusters = [ind_to_clust[ind] for ind in range(len(samples))]\n",
    "        return self.clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcClusterQuality(samples, clust_labels, dist_met=\"bit\", preprocess=None):\n",
    "    \"\"\"\n",
    "    Measures the quality of a set of clusters using silhouette score\n",
    "\n",
    "    Params:\n",
    "        samples (np.ndarray) : sample values that map to labels by index\n",
    "        clust_labels (np.array, list) : cluster labels for each of the samples\n",
    "        dist_met (str, default=\"bit\") : distance metric to compare samples by when calculating silhouette score\n",
    "        preprocess (method, default=None) : function to preprocess samples into sample x value array for easy manipulation\n",
    "\n",
    "    Returns:\n",
    "        silhouette score of clusters (float)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocessing step for Ligands\n",
    "    if preprocess is not None:\n",
    "        samples = preprocess(samples)\n",
    "    \n",
    "    # create n x n distance matrix \n",
    "    dist_mat = np.ones([len(samples), len(samples)])\n",
    "        \n",
    "    for i in range(len(samples)):\n",
    "        for j in range(len(samples)):\n",
    "            dist_mat[i][j] = Clustering().calc_dist(samples[i], samples[j], how=dist_met)\n",
    "    \n",
    "    # calculate silhouette score for each point\n",
    "    all_scores = []\n",
    "    \n",
    "    for index in range(len(samples)):\n",
    "        \n",
    "        # initialize variables in silhouette score calculation\n",
    "        a_i = 0\n",
    "        \n",
    "        # initialize current point i and the cluster that i is in\n",
    "        i = samples[index]\n",
    "        curr_label = clust_labels[index]\n",
    "        \n",
    "        # get a(i) = get sum of distances from point i to all points in the same cluster and divide by n-1\n",
    "        cluster = np.where(np.array(clust_labels) == curr_label)[0]\n",
    "        \n",
    "        # Set clusters of size 1 to have a silhouette score of 0\n",
    "        if len(cluster) == 1:\n",
    "            all_scores.append(0)\n",
    "        else:\n",
    "            # Continue to get a(i). The current point is removed from the calculation\n",
    "            # although it shouldn't matter since it would have a distance of 0\n",
    "            cluster = cluster[cluster != index]\n",
    "            \n",
    "            for curr_clust in cluster:\n",
    "                a_i += dist_mat[index][curr_clust]\n",
    "            \n",
    "            a_i /= len(cluster) # divide by n - 1 (since point has been removed)\n",
    "            \n",
    "            # Get lowest average distance to other clusters, which is equivalent to \n",
    "            # b = sum of distances from point i to all points in the nearest cluster (divided by n)\n",
    "            b_i = np.inf\n",
    "            closest_clust = 0\n",
    "            \n",
    "            for label in clust_labels:\n",
    "                if label != curr_label:\n",
    "                    curr_clust = np.where(np.array(clust_labels) == label)[0]\n",
    "                    b_dist = dist_mat[curr_clust].sum(axis=0)[index]\n",
    "                    b_dist = b_dist / len(curr_clust)\n",
    "                    \n",
    "                    if b_dist < b_i:\n",
    "                        b_i = b_dist\n",
    "            \n",
    "            # calculate silhouette score as (b - a) / max(a, b)\n",
    "            all_scores.append((b_i - a_i) / max(b_i, a_i))\n",
    "\n",
    "    # Average across all silhouette scores. The closer the scores are to 1, the better the cluster\n",
    "    return np.mean(np.array(all_scores))    \n",
    "    \n",
    "def calcClusterSimilarity(labels1, labels2):\n",
    "    \"\"\"\n",
    "    Calculates similarity of one set of clusters to another\n",
    "    \n",
    "    Params:\n",
    "        labels1, labels2 (list, np.array, or other list like object) : set of matching cluster labels from different clustering methods / parameters \n",
    "\n",
    "    Returns: \n",
    "        Jaccard index for \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize varaiables for Jaccard Index calcuation\n",
    "    f_11 = 0 # \"True positive\"\n",
    "    f_10 = 0 # \"False positive\"\n",
    "    f_01 = 0 # \"False negative\"\n",
    "    \n",
    "    # check whether each pair of items found in each cluster\n",
    "    # Jaccard index = f11 / f01 + f10 + f11 = same / total (except negatives)\n",
    "    for i in range(len(labels1)):\n",
    "        for j in range(len(labels2)):\n",
    "            pair_1 = labels1[i] == labels1[j]\n",
    "            pair_2 = labels2[i] == labels2[j]\n",
    "            \n",
    "            if pair_1 and pair_2: # pairs of points are in same cluster in both 1 & 2\n",
    "                f_11 += 1 \n",
    "            elif pair_1 and not pair_2: # pairs of points are in same cluster in 1 but not 2\n",
    "                f_10 +=1\n",
    "            elif pair_2 and not pair_1: # pairs of points are in same cluster in 2 but not 1\n",
    "                f_01 +=1\n",
    "    \n",
    "    # Return 1 if there are no common pairs between the samples. Sklearn's Jaccard metric allows\n",
    "    # user input for either returning 0 or 1 in this case but I just chose 1 for simplicity and\n",
    "    # the clusters are technically very similar in that they don't share any overlap\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/metrics/_classification.py#L642\n",
    "    if (f_01 + f_10 + f_11) == 0: \n",
    "        return 1\n",
    "    \n",
    "    return f_11 / (f_01 + f_10 + f_11)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "A = [1,1,1,0,1]\n",
    "B = [1,0,1,0,1]\n",
    "\n",
    "calcClusterSimilarity(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38235294117647056"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ligands = readLigands(\"./ligand_information.csv\", n=10)\n",
    "\n",
    "clustering = HierarchicalClustering(k=2)\n",
    "clusters1 = clustering.cluster(ligands, preprocess=preprocessLigands)\n",
    "\n",
    "clustering2 = HierarchicalClustering(k=5)\n",
    "clusters2 = clustering2.cluster(ligands, preprocess=preprocessLigands)\n",
    "\n",
    "calcClusterSimilarity(clusters1, clusters2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 3],\n",
       " 1: [1, 2, 4, 13, 17, 29, 46, 47],\n",
       " 2: [5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  16,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  30,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45],\n",
       " 3: [14, 15, 31, 48, 49],\n",
       " 4: [32, 33]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#TODO: create \n",
    "\n",
    "array_dict = {}\n",
    "\n",
    "for item in range(len(array)):\n",
    "    if array[item] in array_dict:\n",
    "        curr_entry = array_dict[array[item]]\n",
    "        curr_entry.append(item)\n",
    "    else:\n",
    "        curr_entry = [item]\n",
    "    \n",
    "    array_dict[array[item]] = curr_entry\n",
    "        \n",
    "array_dict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
